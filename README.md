# Low-Latency Fraud Detection

Real-time fraud detection microservice with calibrated ML models, 5‑minute tail-latency monitoring (P50/P95/P99), and
streaming conformal prediction under drift. Built to be audit-friendly (clear splits, artifacts, and live metrics) and
demo-ready.

## Version & Report

- Version: v1.0.0 (first public cut)
- Report and archived release (Zenodo): https://zenodo.org/records/17571285  
  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17571285.svg)](https://doi.org/10.5281/zenodo.17571285)

## Quickstart

1. python3 -m venv .venv && source .venv/bin/activate
2. make install
3. python -m scripts.train_baseline
4. python -m scripts.evaluate_offline
5. (optional) make stream # generate streaming artifacts locally
6. make serve # http://localhost:8000 (dashboard at /)

## What’s included

- FastAPI service with `/predict`, `/metrics`, `/metrics/offline`, `/metrics/stream`
- Latency instrumentation (server-side) with one timer, 5‑minute percentiles, and background-flushed metrics
- Offline model evaluation (ROC/PR, Brier, NLL) with strict train/calibrate/test protocol
- Streaming conformal prediction (Mondrian, window or exp‑decay), ablations, and artifacts (CSV/PNG/JSON)
- Dashboard: latency cards with sample gating, streaming coverage with class-conditional chips, offline plots

## Live artifacts (PNG/CSV/JSON)

Images are generated by the pipeline and served from `artifacts/`. These are the exact files referenced by the dashboard
and suitable for your report appendix.

- ROC curve: `artifacts/roc.png`
- PR curve: `artifacts/pr.png`
- Reliability (pre): `artifacts/reliability_pre.png`
- Reliability (post): `artifacts/reliability_post.png`
- Score histogram: `artifacts/hist_scores.png`
- Streaming coverage plot: `artifacts/stream_coverage.png`
- Streaming metrics CSV: `artifacts/streaming_metrics.csv`
- Streaming summary JSON: `artifacts/stream_summary.json`

Inline previews:

<p>
  <img src="artifacts/roc.png" alt="ROC" width="720" loading="lazy"/>
</p>
<p>
  <img src="artifacts/pr.png" alt="PR" width="720" loading="lazy"/>
</p>
<p>
  <img src="artifacts/reliability_pre.png" alt="Reliability (pre)" width="720" loading="lazy"/>
</p>
<p>
  <img src="artifacts/reliability_post.png" alt="Reliability (post)" width="720" loading="lazy"/>
</p>
<p>
  <img src="artifacts/stream_coverage.png" alt="Streaming Coverage" width="720" loading="lazy"/>
</p>

## Latency methodology (server view, 5‑minute window)

- Scope: only POST `/predict` with 2xx status. Measured from ASGI request arrival to last byte written.
- Instrumentation: one high‑res timer; convert once to ms. Store uniformly.
- Aggregation: 5‑minute rolling window, ring buffer; cards show P50/P95/P99 and sample sizes. P95 is shown only when
  count_5m ≥ 500; P99 only when count_5m ≥ 10k.
- Background worker: metrics flush (JSON) is debounced off the request path. Optional admin flush endpoint (disabled by
  default).

API response from `/metrics` includes:

```json
{
  "ts": 1762966353.3440263,
  "window_seconds": 300.0,
  "count_30s": 0,
  "count_5m": 0,
  "rps": 0.0,
  "rps_5m": 0.0,
  "p50_ms": 0.0,
  "p95_ms": 0.0,
  "p99_ms": 0.0
}
```

Operational SLOs (demo): server P95 ≤ ~25 ms; P99 < ~60 ms over 5‑minute windows (met in the final config).

## Offline evaluation and calibration (audit-friendly)

Data protocol (strict three-way split):

- Train/Valid: select base model on a held‑out validation split (e.g., AP), then refit on train+valid.
- Calibrate: fit `CalibratedClassifierCV(estimator=base, method="sigmoid", cv="prefit")` on the calibration split only.
- Test: compute ROC‑AUC, PR‑AUC, Brier, and NLL on the test split only.

Notes:

- Reliability diagrams and “post” metrics use the calibrated probabilities from the Platt/sigmoid calibrator (
  monotonic → preserves ROC/PR ranking).
- Extreme class imbalance: calibrated scores typically live in [0, 0.3]; plots are annotated accordingly.
- Outputs are saved to `models/metrics_offline.json` and the PNGs above.

## Cross‑validated baselines (CV)

`scripts/train_baseline.py` also runs a CV sweep on train+valid (no test leakage) for model families like Logistic
Regression, RF/GBDT, SVM, and MLP. Results (ROC‑AUC/PR‑AUC mean±std and chosen hyperparameters) are serialized into
`models/metrics_offline.json` and rendered in the dashboard as “Baselines (CV)”. CV strategy is configurable (
`--cv-mode stratified|temporal|group`).

## Streaming conformal prediction (Mondrian)

We use label‑conditional (Mondrian) conformal prediction with either a fixed window or exponential decay buffer. Metrics
reported:

- Per‑class coverage: fraud (positive) and non‑fraud (negative)
- Overall coverage, average set size, and ambiguous share (set size > 1)
- Violation (target gap): defined as `max(0, target − coverage)`

Final demo configuration (exp‑decay):

```text
--mode exp --decay 0.02 --label_delay 200 --warmup 200
--mondrian_seg label
--alpha_pos 0.039 --alpha_neg 0.12
--pos_slack 0.02
--adapt_gain 0.35 --adapt_gain_pos 0.9 --adapt_start_pos_n 10
--max_events 100000
```

Observed results (5‑minute view, steady state):

- Fraud coverage ≈ 94.67% (target 95% → −0.33 pp; within ±0.5 pp acceptance band)
- Non‑fraud ≈ 97.8%; Overall ≈ 97.8%
- Ambiguity ≈ 29.5%; Average set size ≈ 1.295
- Effective sample size (exp‑decay) ≈ 89

Artifacts:

- Plot: `artifacts/stream_coverage.png`
- CSV: `artifacts/streaming_metrics.csv`
- Summary JSON: `artifacts/stream_summary.json`

Ablations:

- Window size and exp‑decay λ sweeps are available via `POST /metrics/stream/ablate`; results are saved as
  `artifacts/stream_ablation.json` (or CSV fallback). During ablations, adaptation is frozen for consistent comparisons,
  and rows include effective N.

## API

- GET `/` – dashboard
- GET `/health` – status + model metadata
- POST `/predict` – JSON features → `{proba, label, latency_ms}`
- GET `/metrics` – latency window summary with `ts`, `window_seconds`, counts, RPS, P50/P95/P99 (server view)
- GET `/metrics/offline` – offline metrics and artifact paths
- GET `/metrics/stream?limit=K` – recent streaming metrics arrays (includes `coverage_pos`/`coverage_neg`)
- POST `/metrics/stream/ablate` – start ablation with current stream config
- POST `/admin/flush` – optional; disabled unless `ENABLE_ADMIN=true` and `ADMIN_TOKEN` set

## Repro commands (local)

```bash
# 1) Train + artifacts
python -m scripts.train_baseline
python -m scripts.evaluate_offline

# 2) Streaming (final config)
python -m scripts.simulate_stream \
  --mode exp --decay 0.02 --label_delay 200 --warmup 200 \
  --mondrian_seg label \
  --alpha_pos 0.039 --alpha_neg 0.12 \
  --pos_slack 0.02 \
  --adapt_gain 0.35 --adapt_gain_pos 0.9 --adapt_start_pos_n 10 \
  --max_events 100000

# 3) Ablation (reuses current streaming config)
curl -X POST http://localhost:8000/metrics/stream/ablate

# 4) Serve dashboard
make serve  # http://localhost:8000
```

## Layout

- `app/`: FastAPI app, routers, schemas, state, metrics, dashboard assets
- `scripts/`: training, offline eval, streaming simulator, shared utilities
- `tests/`: API, latency, calibration tests
- `data/`, `models/`, `artifacts/`: inputs and outputs

## CI and deploy (Fly.io)

- CI: trains, evaluates offline, simulates streaming (bounded fast path), and runs tests. Artifacts are produced under
  `artifacts/` for publishing.
- Deploy: push to main triggers container build and `flyctl deploy`. Live app (demo):
  `https://low-latency-fraud-detection.fly.dev`.

## Roadmap (v2 - toward CIFRE)

This is v1. The next version will focus on making the system production‑grade:

- Serving & SLOs: shadow→canary→promotion, rollback playbook; drift (PSI/KS) + alerting; P95/P99 SLO‑keeper.
- Systemization: ONNX/int8 path, KServe/Istio Helm; Prometheus/Grafana dashboards.
- Modeling: streaming conformal with weighted/Mondrian variants; Pareto cost‑latency‑risk trade‑offs.
- Reliability: fault injection for MTTD/MTTR evidence; ablations at scale.
- Outreach & packaging: evidence pack (plots, metrics, demo), report v1 and short slide deck.

## Academic context

Coursework project (CNAM Paris). Dataset: Credit Card Fraud (ULB/Kaggle). Report covers calibration, streaming conformal
prediction, and tail-latency SLOs. The README focuses on implementation and how to reproduce plots; methodological
details are in the report.
